{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2ywAbO4RViJ"
   },
   "source": [
    "# Day 1 (Cont'd): Introduction of PyTorch\n",
    ">![PyTorch](https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png)\n",
    "\n",
    "PyTorch is one of the widely used open-source Machine Learning libraries which provides primitives for defining functions on tensors and automatically computing their derivatives. It is powerful and achieved great success for implementing Machine Learning and other algorithms involving a large number of mathematical operations. \n",
    "\n",
    "PyTorch was developed by Meta AI and is one of the most popular Machine Learning libraries on GitHub. \n",
    "PyTorch tensors are similar to NumPy arrays and can additionally be operated on a CUDA-capable GPU or TPU. \n",
    "Thus, PyTorch is mainly used as:\n",
    ">* A tool for tensor computation (with GPU support)\n",
    ">* A Deep Learning (research) platform which uses tape-based automatic differentiation\n",
    "\n",
    "PyTorch is similar to NumPy, so it will feel quite familiar if you have used NumPy before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzBHIUt9RMjY"
   },
   "source": [
    "\n",
    "This tutorial will have a brief introduction about the core components of PyTorch, including\n",
    ">* Basic PyTorch: tensors, computational graph, parameters, gpu support\n",
    ">* Flowchart: build graph, get output, gradient computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PI6AVR0Yv6v"
   },
   "source": [
    "## Tensor\n",
    "\n",
    "Tensors are the basic data structures in PyTorch.\n",
    "Formally, tensors are multilinear maps from vector spaces to the real numbers. In other words, a tensor is a N-dimensional vector, means a tensor can be used to represent N-dimensional datasets.\n",
    "\n",
    "A scalar is a tensor, a vector is a tensor, a matrix is also a tensor, but they are of different dimensions.\n",
    "\n",
    "![tensor](https://cdn-images-1.medium.com/max/1000/1*Wv9adjSwmgl4wLE7lSTRIw.png)\n",
    "\n",
    "As the dimensions keep on increasing, data representation will become more and more complex. On given tensors, we can apply PyTorch operators for processing, which are similar to their counterparts in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NNjGQ3Jgdcyq",
    "outputId": "2022f338-393d-4b9a-d171-261fc11929d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cu121\n",
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "print(torch.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "V9Pe0MqkS_Q1",
    "outputId": "c8486875-584c-4409-f1a5-d13e37bc78e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "(2, 2)\n",
      "[[1. 0. 0. 1.]]\n",
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "### with numpy\n",
    "a = np.eye(2); b = np.ones((2,2))\n",
    "print(np.sum(b))\n",
    "print(a.shape)\n",
    "print(np.reshape(a, (1,4)))\n",
    "print(np.matmul(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "JK-CNI2O0u24",
    "outputId": "5b94264f-ebdd-4c9f-d3a5-7334f0ea221b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "torch.Size([2, 2])\n",
      "tensor([[1., 0., 0., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "### repeat it in PyTorch\n",
    "a = torch.eye(2); b = torch.ones((2,2))\n",
    "print(torch.sum(b)) # or print(b.sum())\n",
    "print(a.size()) # or just use a.shape, like in numpy\n",
    "print(torch.reshape(a, (1,4))) # or print(a.view(1,4))\n",
    "print(torch.mm(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fy18Zi2bNrNu"
   },
   "source": [
    "For more numpy-style matrix operations you can refer to the [pytorch documentation](https://pytorch.org/docs/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryWWU0HZcLkT"
   },
   "source": [
    "## Computational Graph\n",
    "\n",
    "Now we understood what tensors are and itâ€™s time to understand the dynamically created computational graph. Instead of using a static graph concept, such as TensorFlow, PyTorch takes a dynamic graph creation approach. That is, the graph is created on the go. This makes it possible to change the graphs even during runtime.\n",
    "\n",
    "A computational graph is a series of PyTorch operations arranged into a graph.\n",
    "\n",
    "For example, look at the graph below:\n",
    "\n",
    ">![dynamic graph](https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/dynamic_graph.gif)\n",
    "\n",
    "Some rules for the computational graph:\n",
    "1. Start of the graph is always tensors. Therefore, operations can never occur without inputs\n",
    "2. Each operation should accept tensors and then produce new tensors\n",
    "3. Complex operations are in hierarchial order\n",
    "4. Operations in the nodes of the same level are independent of each other\n",
    "\n",
    "### Automatic Differentiation\n",
    "\n",
    "PyTorch uses reverse-mode auto-differentiation in order to compute the gradient of a function with respect to the inputs. Automatic differentiation utilises the chain rule, which allows for calculating complex derivatives by splitting them and recombining them later. Therefore, it is a very useful tool for neural networks.\n",
    "\n",
    "`requires_grad` determines whether PyTorch needs to calculate the gradients with respect to this tensor later in the optimisation steps or not. This argument is set to **False** by default. When we set the parameter `requires_grad=True`, we specify that once the gradient of a tensor, which was built based on this tensor, is calculated, we store the derivative with respect to this tensor in its `grad` attribute. That is, whenever the tensor is used in an operation, PyTorch creates and stores a gradient function for it.\n",
    "In order to do so, each time a new tensor is created by operating on other tensors, the derivative functions of said operations are stored in the new tensor's `grad_fn` attribute.\n",
    "\n",
    "If the `backward()` function of a tensor is called, it computes the gradient of a tensor w.r.t. graph leaves, i.e., it iteratively combines the derivative (`grad_fn`) functions via chain rule until the derivatives w.r.t. all tensors with `requires_grad=True` are calculated. `Attention:` You might need to zero `.grad` attributes or set them to None before calling it. \n",
    "If a tensor is non-scalar (i.e. its data has more than one element) and requires_grad=True, the function additionally requires specifying the gradient argument in the `backward()` function. However, most of the time we only have a scalar output, since we compute the loss for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "y9Kv61dTsc4k",
    "outputId": "eb89a27f-d12d-4970-9089-865bf7cdb6e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4812,  0.4688,  1.3136,  1.2392, -0.3129], requires_grad=True)\n",
      "tensor([ 0.2507, -1.8078, -1.6042,  0.5388, -1.2312])\n",
      "\n",
      "Some Operations\n",
      "tensor(2., grad_fn=<MulBackward0>)\n",
      "tensor([2.4812, 2.4688, 3.3136, 3.2392, 1.6871], grad_fn=<AddBackward0>)\n",
      "tensor([2.2507, 0.1922, 0.3958, 2.5388, 0.7688])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(1., requires_grad=True)\n",
    "b = 2\n",
    "\n",
    "x = torch.randn(5, requires_grad=True)\n",
    "y = torch.randn(5)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "print(\"\\nSome Operations\")\n",
    "print(a * b)\n",
    "print(x + b)\n",
    "print(y + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJgN1t1Esc4m"
   },
   "source": [
    "When we print the output from the above example, we see there is a `grad_fn` parameter. This is the gradient function which is automatically created by PyTorch and is used for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Ys3V6fLIsc4p",
    "outputId": "2a0580c3-e27d-4032-e2ed-bd3d735ea01a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation c = a * b:  tensor(2., grad_fn=<MulBackward0>)\n",
      "Gradient of c w.r.t. a:  tensor(2.)\n",
      "\n",
      "Operation mean(x + b):  tensor(2.6380, grad_fn=<MeanBackward0>)\n",
      "Gradient of w w.r.t. x:  tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n"
     ]
    }
   ],
   "source": [
    "c = a * b\n",
    "# a.grad.zero_()\n",
    "c.backward()\n",
    "print(\"Operation c = a * b: \", c)\n",
    "print(\"Gradient of c w.r.t. a: \", a.grad)\n",
    "\n",
    "w = torch.mean(x + b)\n",
    "# x.grad.zero_()\n",
    "w.backward()\n",
    "print(\"\\nOperation mean(x + b): \", w)\n",
    "print(\"Gradient of w w.r.t. x: \", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EDU9wnUmsc4q",
    "outputId": "1561616c-2e4d-4637-d1a4-33dba1c32a79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation z = a * d:  tensor(2., grad_fn=<MulBackward0>)\n",
      "Gradient:  None\n"
     ]
    }
   ],
   "source": [
    "# Example when no gradient is computed\n",
    "d = torch.tensor(2.)\n",
    "z = a * d\n",
    "z.backward()\n",
    "print(\"Operation z = a * d: \", z)\n",
    "print(\"Gradient: \", d.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbBp_IqUsc4s"
   },
   "source": [
    "### Zeroing out the gradients\n",
    "\n",
    "PyTorch accumulates the gradients on subsequent backward passes, i.e., on every loss.backward() call. This can be beneficial if you want to calculate the gradient summed over multiple mini-batches. However, typically you want to zero out the gradients when you start your training loop so that the parameter update is done correctly. If the gradient is not set to zero before backpropagation, the gradient would be a combination of the old (already used) and the newly calculated gradient.\n",
    "\n",
    "In order to zero out the gradients you can do the following, before calling the `backward()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NsOJ4McIsc4t",
    "outputId": "4102bcf4-e02b-454b-d7e0-d3e0b5d2c487"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad.zero_()\n",
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IL29nxyusc4v"
   },
   "source": [
    "**Note:** When you are using an optimiser from the `torch.optim` package you can use the `zero_grad()` function to zero out all gradients, e.g., `optimiser.zero_grad()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWZr5H_zsc4w"
   },
   "source": [
    "## Custom PyTorch Modules and Parameters\n",
    "\n",
    "PyTorch uses modules to represent neural networks. In this connection, `torch.nn.Module` poses the base class for all neural network modules. Therefore, your models should subclass this class as well.\n",
    "When you need a model which has more complex modules than the already existing ones, you have to define your own custom modules and use them for your desired model.\n",
    "\n",
    "In order to add a parameter to the model parameter list, `torch.nn.Parameter()` has to be used. This way, the tensor will automatically be added to the list. **Attention:** this would not be the case when simply using a normal tensor.\n",
    "\n",
    "For more information please be referred to the [PyTorch modules documentation](https://pytorch.org/docs/stable/notes/modules.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewFtb3FAsc4y"
   },
   "source": [
    "Example code for our own module which should calculate the following formula: $f(x) = ax^2 + bx + c$, with $x$ being the input data and $a$, $b$, $c$ being the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Bwztd-1Msc4z",
    "outputId": "627d6496-7407-472e-f96e-950e2dbb515f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module:  Polynomial2(a: torch.Size([1]), b: torch.Size([1]), c: torch.Size([1]))\n",
      "\n",
      "Module Parameters:\n",
      " [Parameter containing:\n",
      "tensor([-0.2613], requires_grad=True), Parameter containing:\n",
      "tensor([0.0803], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3050], requires_grad=True)]\n",
      "tensor([-12.5484], grad_fn=<AddBackward0>)\n",
      "tensor([-12.5484], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class CustomModule(torch.nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn((size)))\n",
    "        self.b = torch.nn.Parameter(torch.randn((size)))\n",
    "        self.c = torch.nn.Parameter(torch.randn((size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.a * x**2 + self.b * x + self.c\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Polynomial2(a: {self.a.shape}, b: {self.b.shape}, c: {self.c.shape})'\n",
    "        \n",
    "\n",
    "module = CustomModule(size=1)\n",
    "print(\"Module: \", module) # Uses our string representation\n",
    "print(\"\\nModule Parameters:\\n\", list(module.parameters()))\n",
    "output = module(7)\n",
    "print(module(7))\n",
    "print(module.forward(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gl4fEYmvsc41"
   },
   "source": [
    "#### Further explanations w.r.t. the above custom module\n",
    "\n",
    "* The python `__init__()` function initialises a newly created object and uses the passed arguments in order to do so. So, $\\text{__init__()}$ is called when the class is called to initialise the instance.\n",
    "* The python $\\text{__call__()}$ function is called when the instance is called. It allows the class's instance to be called as a function. Moreover, **it is already defined** in `nn.Module`, will register all hooks, and call your $\\text{forward()}$ function. That is, your module can be used as a function. For instance, above we can simply say `output = module(7)` instead of `output = module.forward(7)`.\n",
    "* The python $\\text{__repr__()}$ function: returns the object representation in string format. It is called when the repr()-function is applied to the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsqa-45ksc42"
   },
   "source": [
    "## Conversion from NumPy to PyTorch and back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfacCxCjx4p6"
   },
   "source": [
    "All previous examples have manually defined tensors for input data, but how can we input external data into PyTorch?\n",
    "Simple solution is to use from_numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VJ0I2h34yG9b",
    "outputId": "c7356819-c7ac-47a8-871a-756eb088aca0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.43565476 0.02605827 0.58459374]\n",
      " [0.97909302 0.48270368 0.14847264]\n",
      " [0.52390551 0.09153113 0.09173771]]\n",
      "tensor([[0.4357, 0.0261, 0.5846],\n",
      "        [0.9791, 0.4827, 0.1485],\n",
      "        [0.5239, 0.0915, 0.0917]], dtype=torch.float64)\n",
      "[[0.43565476 0.02605827 0.58459374]\n",
      " [0.97909302 0.48270368 0.14847264]\n",
      " [0.52390551 0.09153113 0.09173771]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.random((3,3))\n",
    "print(a)\n",
    "\n",
    "# Convert numpy array to tensor\n",
    "ta = torch.from_numpy(a) \n",
    "print(ta)\n",
    "# or ta = torch.tensor(a)\n",
    "# or ta = torch.Tensor(a)\n",
    "\n",
    "# Convert back\n",
    "na = ta.detach().cpu().numpy()\n",
    "print(na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32bFugL7sc45"
   },
   "source": [
    "## GPU support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GaWpRTHJsc46",
    "outputId": "22c07a92-4442-4fc5-c084-cf0c15fa4a43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Check for cuda availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create tensor on device\n",
    "x = torch.ones(5, device=device)\n",
    "\n",
    "# Move tensor to device\n",
    "x = x.to(device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-LQtGHZKQ6L"
   },
   "source": [
    "## Exercise 2:\n",
    "For the following 10*10 array A, \n",
    "\n",
    "\n",
    "\n",
    "> [[0.69145505, 0.86931882, 0.88576413, 0.82707554, 0.94754421, 0.54767962,  0.51818679, 0.27907936, 0.95212406, 0.22750068],\n",
    "\n",
    "> [0.345638,   0.16172159, 0.87807572, 0.38444467, 0.84255332, 0.69666159,  0.43339905, 0.91927538, 0.58666126, 0.83215206],\n",
    "\n",
    ">[0.91359442, 0.06356911, 0.37205853, 0.18242315, 0.37961342, 0.93335263,  0.34068447, 0.48598708, 0.24260729, 0.70004846],\n",
    "\n",
    ">[0.75245372, 0.64147803, 0.84013461, 0.6152693,  0.02235612, 0.4492574,  0.55206705, 0.69409179, 0.1666939,  0.67387225],\n",
    "\n",
    ">[0.30664677, 0.87559232, 0.07164895, 0.85516997, 0.77945438, 0.51948711,  0.18721151, 0.7690967 , 0.53605078, 0.55431431],\n",
    "\n",
    ">[0.1750064,  0.95009262, 0.57121048, 0.87359026, 0.05715099, 0.43202169,  0.3648696,  0.24367817, 0.06807447, 0.46999578],\n",
    "\n",
    ">[0.41121198, 0.10125657, 0.0869751,  0.91816382, 0.01738795, 0.19420588,  0.00127754, 0.19281699, 0.56083174, 0.55424236],\n",
    "\n",
    ">[0.34467108, 0.18352578, 0.69203741, 0.48087863, 0.39596428, 0.28107969,  0.09727506, 0.11236618, 0.82687268, 0.22700161],\n",
    "\n",
    ">[0.92788092, 0.87184167, 0.72492497, 0.94086364, 0.86998108, 0.35178978,  0.45463869, 0.0242793,  0.75607483, 0.21317889],\n",
    "\n",
    ">[0.15680697, 0.13109825, 0.93463861, 0.78143659, 0.30680001, 0.67935342,  0.3583568,  0.7522564,  0.19810852, 0.22378965]],\n",
    "\n",
    "\n",
    "\n",
    "answer the following questions by programming with PyTorch. Again avoid any explicit implementations of for-/while-loops or list comprehensions.\n",
    "\n",
    "\n",
    "1.   What is the row index in A that has the largest last element? \n",
    "2.   What is the row index in A that has the second largest last element?\n",
    "3.   What is the row index in A that has a row sum greater than 5?\n",
    "4.   What is the sum of all elements of the form A\\[i, i+1] in A?\n",
    "5.   Multiply elementwise every row in matrix A by the vector w \\[0.49039597 0.73424538 0.08249155 0.0488797  0.62525918 0.29331343 0.76435348 0.68825002 0.53465669 0.3399619], and then sum the results for every row\n",
    "6.   Do the above task in 5. using matrix multiplication\n",
    "7.   Set all element of A that are larger than 0.5 to 0. What is the sum of the resulting matrix?\n",
    "8.   Subtract 1 from all elements of A that are smaller than 0.5. What is the sum of the resulting matrix?\n",
    "9.   What is the sum of all element in A, that are smaller than their column's mean?\n",
    "10.  Create a diagonal matrix B of size 10*10, with B\\[i,i] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YGtlrGkdsc49",
    "outputId": "fb3b9afd-359b-4dcf-8a2a-c87feda4cf92",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6915, 0.8693, 0.8858, 0.8271, 0.9475, 0.5477, 0.5182, 0.2791, 0.9521,\n",
      "         0.2275],\n",
      "        [0.3456, 0.1617, 0.8781, 0.3844, 0.8426, 0.6967, 0.4334, 0.9193, 0.5867,\n",
      "         0.8322],\n",
      "        [0.9136, 0.0636, 0.3721, 0.1824, 0.3796, 0.9334, 0.3407, 0.4860, 0.2426,\n",
      "         0.7000],\n",
      "        [0.7525, 0.6415, 0.8401, 0.6153, 0.0224, 0.4493, 0.5521, 0.6941, 0.1667,\n",
      "         0.6739],\n",
      "        [0.3066, 0.8756, 0.0716, 0.8552, 0.7795, 0.5195, 0.1872, 0.7691, 0.5361,\n",
      "         0.5543],\n",
      "        [0.1750, 0.9501, 0.5712, 0.8736, 0.0572, 0.4320, 0.3649, 0.2437, 0.0681,\n",
      "         0.4700],\n",
      "        [0.4112, 0.1013, 0.0870, 0.9182, 0.0174, 0.1942, 0.0013, 0.1928, 0.5608,\n",
      "         0.5542],\n",
      "        [0.3447, 0.1835, 0.6920, 0.4809, 0.3960, 0.2811, 0.0973, 0.1124, 0.8269,\n",
      "         0.2270],\n",
      "        [0.9279, 0.8718, 0.7249, 0.9409, 0.8700, 0.3518, 0.4546, 0.0243, 0.7561,\n",
      "         0.2132],\n",
      "        [0.1568, 0.1311, 0.9346, 0.7814, 0.3068, 0.6794, 0.3584, 0.7523, 0.1981,\n",
      "         0.2238]])\n",
      "tensor([0.4904, 0.7342, 0.0825, 0.0489, 0.6253, 0.2933, 0.7644, 0.6883, 0.5347,\n",
      "        0.3400])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "A = torch.tensor([[0.69145505, 0.86931882, 0.88576413, 0.82707554, 0.94754421, 0.54767962,  0.51818679, 0.27907936, 0.95212406, 0.22750068],\n",
    "[0.345638,   0.16172159, 0.87807572, 0.38444467, 0.84255332, 0.69666159,  0.43339905, 0.91927538, 0.58666126, 0.83215206],\n",
    "[0.91359442, 0.06356911, 0.37205853, 0.18242315, 0.37961342, 0.93335263,  0.34068447, 0.48598708, 0.24260729, 0.70004846],\n",
    "[0.75245372, 0.64147803, 0.84013461, 0.6152693,  0.02235612, 0.4492574,  0.55206705, 0.69409179, 0.1666939,  0.67387225],\n",
    "[0.30664677, 0.87559232, 0.07164895, 0.85516997, 0.77945438, 0.51948711,  0.18721151, 0.7690967 , 0.53605078, 0.55431431],\n",
    "[0.1750064,  0.95009262, 0.57121048, 0.87359026, 0.05715099, 0.43202169,  0.3648696,  0.24367817, 0.06807447, 0.46999578],\n",
    "[0.41121198, 0.10125657, 0.0869751,  0.91816382, 0.01738795, 0.19420588,  0.00127754, 0.19281699, 0.56083174, 0.55424236],\n",
    "[0.34467108, 0.18352578, 0.69203741, 0.48087863, 0.39596428, 0.28107969,  0.09727506, 0.11236618, 0.82687268, 0.22700161],\n",
    "[0.92788092, 0.87184167, 0.72492497, 0.94086364, 0.86998108, 0.35178978,  0.45463869, 0.0242793,  0.75607483, 0.21317889],\n",
    "[0.15680697, 0.13109825, 0.93463861, 0.78143659, 0.30680001, 0.67935342,  0.3583568,  0.7522564,  0.19810852, 0.22378965]])\n",
    "\n",
    "\n",
    "w = torch.tensor([0.49039597, 0.73424538, 0.08249155, 0.0488797,  0.62525918, 0.29331343, 0.76435348, 0.68825002, 0.53465669, 0.3399619])\n",
    "\n",
    "print(A)\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zx84dCVssc4-"
   },
   "source": [
    "1. What is the row index in A that has the largest last element?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JL7nRNkdsc4_",
    "outputId": "a3810395-a601-46ba-9468-616605410849",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "row_index_with_largest_last_element = torch.argmax(A[:,-1])\n",
    "print(row_index_with_largest_last_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBwDtwlVsc5A"
   },
   "source": [
    "2. What is the row index in A that has the second largest last element?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EOglZNftsc5A",
    "outputId": "ae68ef8f-bd2f-49d1-d88c-3c8d745d991f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "# inefficient way\n",
    "sorted_indices = torch.argsort(A[:,-1])\n",
    "print(sorted_indices[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMcnXECWsc5B"
   },
   "source": [
    "3. What is the row index in A that has a row sum greater than 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AVblUDZ5sc5C",
    "outputId": "328d8129-558c-4e48-9a32-cefc6a69c26b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 3, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "# note: several indices!\n",
    "row_with_sum_greater_5 = torch.where(torch.sum(A, dim=1) > 5)[0]\n",
    "print(row_with_sum_greater_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6tJFn0Esc5C"
   },
   "source": [
    "4. What is the sum of all elements of the form A\\[i, i+1] in A?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "agqdqF-gsc5D",
    "outputId": "89720047-7d78-4779-a514-d964b514d801",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.0694)\n"
     ]
    }
   ],
   "source": [
    "assert torch.isclose(A[0,1], torch.diag(A, diagonal=1)[0])\n",
    "print(A.diag(diagonal=1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvG-eGs_sc5D"
   },
   "source": [
    "5. Multiply elementwise every row in matrix A by the vector w \\[0.49039597 0.73424538 0.08249155 0.0488797 0.62525918 0.29331343 0.76435348 0.68825002 0.53465669 0.3399619], and then sum the results for every row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-rFKHAYEsc5E",
    "outputId": "d0dd91c9-889f-4d0c-eedb-5498579bd6da",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3391, 0.6383, 0.0731, 0.0404, 0.5925, 0.1606, 0.3961, 0.1921, 0.5091,\n",
      "         0.0773],\n",
      "        [0.1695, 0.1187, 0.0724, 0.0188, 0.5268, 0.2043, 0.3313, 0.6327, 0.3137,\n",
      "         0.2829],\n",
      "        [0.4480, 0.0467, 0.0307, 0.0089, 0.2374, 0.2738, 0.2604, 0.3345, 0.1297,\n",
      "         0.2380],\n",
      "        [0.3690, 0.4710, 0.0693, 0.0301, 0.0140, 0.1318, 0.4220, 0.4777, 0.0891,\n",
      "         0.2291],\n",
      "        [0.1504, 0.6429, 0.0059, 0.0418, 0.4874, 0.1524, 0.1431, 0.5293, 0.2866,\n",
      "         0.1884],\n",
      "        [0.0858, 0.6976, 0.0471, 0.0427, 0.0357, 0.1267, 0.2789, 0.1677, 0.0364,\n",
      "         0.1598],\n",
      "        [0.2017, 0.0743, 0.0072, 0.0449, 0.0109, 0.0570, 0.0010, 0.1327, 0.2999,\n",
      "         0.1884],\n",
      "        [0.1690, 0.1348, 0.0571, 0.0235, 0.2476, 0.0824, 0.0744, 0.0773, 0.4421,\n",
      "         0.0772],\n",
      "        [0.4550, 0.6401, 0.0598, 0.0460, 0.5440, 0.1032, 0.3475, 0.0167, 0.4042,\n",
      "         0.0725],\n",
      "        [0.0769, 0.0963, 0.0771, 0.0382, 0.1918, 0.1993, 0.2739, 0.5177, 0.1059,\n",
      "         0.0761]])\n",
      "tensor([3.0185, 2.6711, 2.0080, 2.3030, 2.6282, 1.6785, 1.0178, 1.3853, 2.6890,\n",
      "        1.6532])\n"
     ]
    }
   ],
   "source": [
    "result1 = A[:,:] * w[None,:]\n",
    "print(result1)\n",
    "result2 = result1.sum(axis=1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TsVssu3sc5F"
   },
   "source": [
    "6. Do the above task in 5. using matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "TT3ELckRsc5F",
    "outputId": "8db4d204-5190-4f95-8c42-7b408974617a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.0185, 2.6711, 2.0080, 2.3030, 2.6282, 1.6785, 1.0178, 1.3853, 2.6890,\n",
      "        1.6532])\n"
     ]
    }
   ],
   "source": [
    "print(A @ w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz24kOFgsc5G"
   },
   "source": [
    "7. Set all elements of A that are larger than 0.5 to 0. What is the sum of the resulting matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-Pd2qc8Vsc5G",
    "outputId": "bc635bae-4b72-45e9-cf9d-8fbc8a239634",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.4601)\n"
     ]
    }
   ],
   "source": [
    "newA = A.detach().clone()\n",
    "newA[newA > 0.5] = 0\n",
    "print(newA.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdhkue7xsc5H"
   },
   "source": [
    "8. Subtract 1 from all elements of A that are smaller than 0.5. What is the sum of the resulting matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "hiKkzxutsc5H",
    "outputId": "f98cc2fd-b736-46a1-f5b8-b1fa1b316bd2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.1536)\n"
     ]
    }
   ],
   "source": [
    "newA = A.detach().clone()\n",
    "newA[newA < 0.5] -= 1\n",
    "print(newA.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lecmC561sc5I"
   },
   "source": [
    "9. What is the sum of all elements in A, that are smaller than their column's mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "iLZ0wWZBsc5I",
    "outputId": "98f27332-ddd6-4a59-d934-19e88d839a54",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7386)\n"
     ]
    }
   ],
   "source": [
    "column_mean = A.mean(axis=0)\n",
    "indices_smaller = torch.where(A[:,:] < column_mean[None,:])\n",
    "print(A[indices_smaller].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWMWKxgmsc5J"
   },
   "source": [
    "10. Create a diagonal matrix B of size 10*10, with B\\[i,i] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WOx3W9g1sc5J",
    "outputId": "7c009ebf-97b5-4a0d-e3d2-3b78ca211820",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 2, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 3, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 4, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 5, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 6, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 7, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 8, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 9]])\n"
     ]
    }
   ],
   "source": [
    "B = torch.arange(10).diag()\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "gl4fEYmvsc41"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "9511b72e2f0dc8789a87876b871a2c068f47e3c15d5f47ed9d92a201978a3866"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
