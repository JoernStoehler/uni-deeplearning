{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7goS-dvwgiE"
   },
   "source": [
    "In this exercise, you will code a regression model from scratch, to predict housing prices in California. \n",
    "\n",
    "Each record in the data (x) is comprised of 9 fields which include the size of the apartment in $m^2$, number of rooms, the location, etc.\n",
    "\n",
    "The data is split into a training and test set. There are 8 features for one data entry (8 columns for one row) and the targets (labels) are the median values of the houses at a location in $k$.\n",
    "\n",
    "The model will be a linear regression model: \n",
    "$f_w (x) = w_1 x_1 + ... + w_{8} x_{8} + b$ \n",
    "where w,b are the model parameters we want to learn using gradient descent. \n",
    "\n",
    "The data can be downloaded by executing the already given cell below. You can find more information about the dataset on https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset .\n",
    "\n",
    "Steps: \n",
    "\n",
    "1. Implement a function to compute the loss: loss = (prediction - y)$^2$. Note that this function should calculate the average loss over all training samples. In short: implement a function which calculates the MSE-loss.\n",
    "2. Plot the predictions over the ground truth using `matplotlib`. Label the x and y axis appropriately and show the individual points **without** connecting them with lines.\n",
    "3. Implement the body functions of the `__init__` and `forward` methods of `LinearRegressor` to hold the model parameters w and b (both initialised using a normal distribution) and implement its correct linear regression equation function. Instantiate your module and show its trainable parameters to verify that w and b are within them.\n",
    "4.  Implement the body of the following function to perform one training step, given a torch module (which contains the variables of your model), the training data and labels, as well as the optimizer. Use your MSEloss function to compute the loss and back-propagate the gradients using `.backward()`. Update the model weights using `optimizer.step()`. Remember to reset the gradients before the step.\n",
    "5. Implement the body of the function for training and evaluating given a number of training steps. After every 1000 training steps the function should optionally (i.e., controlled via the `verbose` flag) output the loss on the training set, as well as the test set. The function should return the test set loss on the **last** step.\n",
    "6. Instantiate your model again and wrap it in a suitable stochastic gradient descent optimizer. Use a learning rate of 0.001. Run your function for training and evaluation for 10,000 training steps. At the end of it, show the trained model parameters. Compare the final loss to your baseline loss.\n",
    "7. Plot the test set predictions over the test set ground truth, and additionally compute the loss and the Pearson correlation (see `sklearn` cell).\n",
    "8. Do not feed the entire training set for every iteration (gradient descent), but rather a random subset of 32 training examples (this is called a `batch` and the optimization process `stochastic` gradient descent). Use the given function template and implement its body. Run the function for the same number of iterations and with the same learning rate. Show its loss and plot the predictions. Compare with your baseline and full-batch gradient descent. `Hint:` Implement without replacement, i.e., each sample is taken only once. In order to train a model from scratch, you need to re-instantiate it and wrap it in a new optimizer.\n",
    "9. Benchmark full-batch gradient descent and stochastic gradient descent for a set of different learning rates ($[.1, .01, .001, .0001, .00001]$). Use 10k iterations. Print, plot, and compare the losses on the last step for each training algorithm and each learning rate. What do you observe? What does it mean? Find a proper way to present the information in the plots such that your takeaway message is clear!\n",
    "10. Benchmark full-batch gradient descent and stochastic gradient descent for different iterations ($[10, 100, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]$). Keep the learning rate fixed to $0.0001$. Print, plot, and compare the loss on the last step for each training algorithm and the different number of iterations. What do you observe? What does it mean? Find a proper way to present the information in the plots such that your takeaway message is clear!\n",
    "11. Fill in the class body to implement SGD without automatic differentiation. Do not use torch.nn.Parameter and instead implement a manual backward call. Fill in the new step function that does not use autograd but calls this custom backward call. The gradients of the two parameters can be computed by using the following formulas: $\\delta w = 2 \\frac{1}{N} \\sum_{i=1}^{N}x_i (\\hat{y}_i-y_i)$ and $\\delta b = 2 \\frac{1}{N} \\sum_{i=1}^{N}(\\hat{y}_i-y_i)$, where $N$ is the batch size and $\\hat{y}_i$ the prediction for the $i^{th}$ instance. Test the implementation with the call to training already given. Plot the predictions on the test set and compute the MSELoss and Pearson R.\n",
    "\n",
    "`Note:` Do not use loops for the backpropagation in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import necessary functions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import scipy\n",
    "import torch\n",
    "import typing\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "print(\"numpy: \" + np.__version__)\n",
    "print(\"torch: \" + torch.__version__)\n",
    "print(\"sklearn: \" + sklearn.__version__)\n",
    "print(\"scipy: \" + scipy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load and standardise the features (x_train and x_test), such that the mean is 0 and the standard deviation is 1 for each of the 8 features, i.e. standardise each column. For this purpose, we use the following formula: $x_{norm} = \\frac{x - \\mu}{\\sigma} $, where $\\mu$ is the mean and $\\sigma$ is the standard deviation. We also need to normalise the labels (y_train and y_test) in the range [0-1]. This can be done by applying the following formula: $y_{norm} = \\frac{y-\\min{y}}{\\max{y}-\\min{y}}$. The normalisation parameters for both features and labels are computed on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the data using sklearn\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "\n",
    "# Cast to 32 bits which is the default in torch\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "# Split the data into 80% training and 20% test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"y_train shape: \" + str(y_train.shape))\n",
    "print(\"y_test shape: \" + str(y_test.shape))\n",
    "\n",
    "# Compute feature statistics on training set\n",
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)\n",
    "\n",
    "# Normalise training set features\n",
    "X_train = (X_train - mean) / std\n",
    "print(f'Feature mean (train): {X_train.mean()}')\n",
    "print(f'Feature std (train): {X_train.std()}')\n",
    "\n",
    "# Normalise test set features\n",
    "X_test = (X_test - mean) / std\n",
    "print(f'Feature mean (test): {X_test.mean()}')\n",
    "print(f'Feature std (test): {X_test.std()}')\n",
    "\n",
    "# Compute label statistics on training set\n",
    "maximum = y_train.max()\n",
    "minimum = y_train.min()\n",
    "\n",
    "# Normalise training set labels\n",
    "y_train = (y_train - minimum) / (maximum-minimum)\n",
    "print(f'Label max (train): {y_train.max()}')\n",
    "print(f'Label min (train): {y_train.min()}')\n",
    "\n",
    "# Normalise test set labels\n",
    "y_test = (y_test - minimum) / (maximum-minimum)\n",
    "print(f'Label max (test): {y_test.max()}')\n",
    "print(f'Label min (test): {y_test.min()}')\n",
    "\n",
    "# Convert everything from numpy arrays to tensors:\n",
    "X_train = torch.from_numpy(X_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a `LinearRegression` baseline from `sklearn`. This should serve as your reference for what is expected by a state-of-the-art implementation. Your goal throughout this day is to get similar (or maybe even better!) results from the models you implement yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = LinearRegression()\n",
    "baseline.fit(\n",
    "    X_train.numpy(), \n",
    "    y_train.numpy()\n",
    ")\n",
    "y_hat = baseline.predict(X_test.numpy())\n",
    "R = scipy.stats.pearsonr(y_hat, y_test)[0]\n",
    "print(f'Baseline Pearson R: {R}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement a function to compute the loss: loss = (prediction - y)$^2$. Note that this function should calculate the average loss over all training samples. In short: implement a function which calculates the MSE-loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSEloss(prediction, label):\n",
    "    return ((prediction - label) ** 2).mean()\n",
    "\n",
    "loss = MSEloss(torch.from_numpy(y_hat), y_test)\n",
    "print(f'Baseline MSE: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Plot the predictions over the ground truth using `matplotlib`. Label the x and y axis appropriately and show the individual points **without** connecting them with lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Plot the predictions over the ground truth using `matplotlib`. Label the x and y axis appropriately and show the individual points **without** connecting them with lines.\n",
    "plt.scatter(y_test, y_hat)\n",
    "plt.xlabel('Ground truth')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Predictions vs Ground truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement the body functions of the `__init__` and `forward` methods of `LinearRegressor` to hold the model parameters w and b (both initialised using a normal distribution) and implement its correct linear regression equation function (using a matrix multiplication). Instantiate your module and show its trainable parameters to verify that w and b are within them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressor(torch.nn.Module):\n",
    "    r\"\"\"Class implementing linear regression model.\n",
    "    \n",
    "    Args:\n",
    "        num_features: number of input features\n",
    "        num_outputs: number of output labels\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features: int, num_outputs: int = 1):\n",
    "        super(LinearRegressor, self).__init__()\n",
    "        self.w = torch.nn.Parameter(torch.randn(num_features, num_outputs)) # [num_features, num_outputs]\n",
    "        self.b = torch.nn.Parameter(torch.randn(num_outputs)) # [num_outputs]\n",
    "        \n",
    "    def forward(self, x): # [batch, num_features] -> [batch, num_outputs]\n",
    "        return torch.mm(x, self.w) + self.b\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'LinearRegressor(w: {self.w.shape}, b: {self.b.shape})'\n",
    "    \n",
    "# instantiate the model and show its trainable parameters\n",
    "model = LinearRegressor(num_features=X_train.shape[1], num_outputs=1)\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'{name}: {param.shape} requires_grad={param.requires_grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  Implement the body of the following function to perform one training step, given a torch module (which contains the variables of your model), the training data and labels, as well as the optimizer. Use your MSEloss function to compute the loss and back-propagate the gradients using `.backward()`. Update the model weights using `optimizer.step()`. Remember to reset the gradients before the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autograd_train_step(\n",
    "    model: torch.nn.Module, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    X: torch.Tensor, \n",
    "    y: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"Single autograd training step.\n",
    "    \n",
    "    Function should use MSEloss defined above to compute loss,\n",
    "    then perform a single optimization step to update model weights,\n",
    "    and finally return the resulting loss.\n",
    "    \n",
    "    Args:\n",
    "        model: pytorch model to be trained\n",
    "        optimizer: optimizer wrapping pytorch model\n",
    "        X: pytorch tensor containing features\n",
    "        y: pytorch tensor containing labels\n",
    "    Returns:\n",
    "        torch.Tensor: loss at current step\n",
    "    \"\"\"\n",
    "    y_hat = model(X).squeeze()\n",
    "    loss = MSEloss(y_hat, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss # training loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Implement the body of the function for training and evaluating given a number of training steps. After every 1000 training steps the function should optionally (i.e., controlled via the `verbose` flag) output the loss on the training set, as well as the test set. The function should return the test set loss on the **last** step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    model: torch.nn.Module, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    X_train: torch.Tensor, \n",
    "    y_train: torch.Tensor, \n",
    "    X_test: torch.Tensor, \n",
    "    y_test: torch.Tensor, \n",
    "    iterations: int, \n",
    "    train_step: typing.Callable,\n",
    "    verbose: bool = False\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"Train and evaluate model.\n",
    "    \n",
    "    Args:\n",
    "        model: pytorch model to be trained\n",
    "        optimizer: optimizer wrapping pytorch model\n",
    "        X_train: pytorch tensor containing training features\n",
    "        y_train: pytorch tensor containing training labels\n",
    "        X_test: pytorch tensor containing test features\n",
    "        y_test: pytorch tensor containing test labels\n",
    "        iterations: number of iterations for gradient descent\n",
    "        train_step: function implementing single training step\n",
    "        verbose: flag controlling whether output should be printed at every iteration\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: loss function at the last step\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(iterations):\n",
    "        train_loss = train_step(model, optimizer, X_train, y_train)\n",
    "        if verbose and i % 1000 == 999:\n",
    "            test_loss = MSEloss(model(X_test), y_test)\n",
    "            print(f'Iteration {i+1}, train_loss: {train_loss} test_loss: {test_loss}')\n",
    "    \n",
    "    test_loss = MSEloss(model(X_test), y_test)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Instantiate your model again and wrap it in a suitable stochastic gradient descent optimizer. Use a learning rate of 0.001. Run your function for training and evaluation for 10,000 training steps. At the end of it, show the trained model parameters. Compare the final loss to your baseline loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressor(num_features=X_train.shape[1], num_outputs=1)\n",
    "\n",
    "# move to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training on {device}')\n",
    "model.to(device)\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "test_loss = train_and_evaluate(\n",
    "    model, \n",
    "    optimizer, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test, \n",
    "    y_test, \n",
    "    iterations=10_000, \n",
    "    train_step=autograd_train_step,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# show trained parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'{name}: {param.data}')\n",
    "\n",
    "# compare final loss to baseline loss\n",
    "print(f'Final test MSE loss: {test_loss}')\n",
    "print(f'Baseline test MSE loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Plot the test set predictions over the test set ground truth, and additionally compute the loss and the Pearson correlation (see `sklearn` cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Plot the test set predictions over the test set ground truth, and additionally compute the loss and the Pearson correlation (see `sklearn` cell).\n",
    "y_hat = model(X_test).detach()\n",
    "plt.scatter(y_test.cpu(), y_hat.cpu())\n",
    "plt.xlabel('Ground truth')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Predictions vs Ground truth')\n",
    "plt.show()\n",
    "\n",
    "test_R = scipy.stats.pearsonr(y_hat[:,0].cpu(), y_test.cpu())[0]\n",
    "print(f'Pearson R: {test_R}')\n",
    "print(f'Baseline Pearson R: {R}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Do not feed the entire training set for every iteration (gradient descent), but rather a random subset of 32 training examples (this is called a `batch` and the optimization process stochastic (or rather mini-batch) gradient descent). Use the given function template and implement its body. Run the function for the same number of iterations and with the same learning rate. Show its loss and plot the predictions. Compare with your baseline and full-batch gradient descent. In order to train a model from scratch, you need to re-instantiate it and wrap it in a new optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_on_batches(\n",
    "    model: torch.nn.Module, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    X_train: torch.Tensor, \n",
    "    y_train: torch.Tensor, \n",
    "    X_test: torch.Tensor, \n",
    "    y_test: torch.Tensor, \n",
    "    iterations: int,\n",
    "    batch_size: int,\n",
    "    train_step: typing.Callable,\n",
    "    verbose: bool = False\n",
    "):\n",
    "    r\"\"\"Train and evaluate model on batches.\n",
    "    \n",
    "    Args:\n",
    "        model: pytorch model to be trained\n",
    "        optimizer: optimizer wrapping pytorch model\n",
    "        X_train: pytorch tensor containing training features\n",
    "        y_train: pytorch tensor containing training labels\n",
    "        X_test: pytorch tensor containing test features\n",
    "        y_test: pytorch tensor containing test labels\n",
    "        iterations: number of iterations for gradient descent\n",
    "        train_step: function implementing single training step\n",
    "        verbose: flag controlling whether output should be printed at every iteration\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: loss function at the last step\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(iterations):\n",
    "        indices = torch.randperm(X_train.shape[0])[:batch_size]\n",
    "        train_loss = train_step(model, optimizer, X_train[indices], y_train[indices])\n",
    "        if verbose and i % 1000 == 999:\n",
    "            test_loss = MSEloss(model(X_test), y_test)\n",
    "            print(f'Iteration {i+1}, train_loss: {train_loss} test_loss: {test_loss}')\n",
    "    \n",
    "    test_loss = MSEloss(model(X_test), y_test)\n",
    "    return test_loss\n",
    "\n",
    "model_batch = LinearRegressor(num_features=X_train.shape[1], num_outputs=1).to(device)\n",
    "optimizer_batch = torch.optim.SGD(model_batch.parameters(), lr=0.001)\n",
    "test_loss_batch = train_and_evaluate_on_batches(\n",
    "    model_batch,\n",
    "    optimizer_batch,\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test, \n",
    "    y_test, \n",
    "    iterations=10_000, \n",
    "    batch_size=32,\n",
    "    train_step=autograd_train_step,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions\n",
    "y_hat_batch = model_batch(X_test).detach()\n",
    "plt.scatter(y_test.cpu(), baseline.predict(X_test.cpu().numpy()), label='Baseline', alpha=0.5)\n",
    "plt.scatter(y_test.cpu(), y_hat.cpu(), label='Full batch', alpha=0.5)\n",
    "plt.scatter(y_test.cpu(), y_hat_batch.cpu(), label='Batched', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel('Ground truth')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Predictions vs Ground truth')\n",
    "plt.show()\n",
    "\n",
    "# compare baseline, full-batch and batch losses\n",
    "print(f'Baseline test MSE loss: {loss}')\n",
    "print(f'Full batch test MSE loss: {test_loss}')\n",
    "print(f'Batched test MSE loss: {test_loss_batch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Benchmark full-batch gradient descent and stochastic gradient descent for a set of different learning rates ($[.1, .01, .001, .0001, .00001]$). Use 10k iterations. Print, plot, and compare the losses on the last step for each training algorithm and each learning rate. What do you observe? What does it mean? Find a proper way to present the information in the plots such that your takeaway message is clear!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_lr = [.1, .01, .001, .0001, .00001]\n",
    "iterations = 10_000\n",
    "batch_size = 32\n",
    "\n",
    "trained_models = {\"full-batch\": {}, \"batched\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched training\n",
    "for lr in set_of_lr:\n",
    "    print(f'Batched training with lr={lr}')\n",
    "    model_batch = LinearRegressor(num_features=X_train.shape[1], num_outputs=1).to(device)\n",
    "    optimizer_batch = torch.optim.SGD(model_batch.parameters(), lr=lr)\n",
    "    test_loss_batch = train_and_evaluate_on_batches(\n",
    "        model_batch,\n",
    "        optimizer_batch,\n",
    "        X_train, \n",
    "        y_train, \n",
    "        X_test, \n",
    "        y_test, \n",
    "        iterations=iterations, \n",
    "        batch_size=batch_size,\n",
    "        train_step=autograd_train_step,\n",
    "        verbose=False\n",
    "    )\n",
    "    trained_models[\"batched\"][lr] = {\n",
    "        \"model\": model_batch,\n",
    "        \"last_test_loss\": test_loss_batch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full-batch training\n",
    "for lr in set_of_lr:\n",
    "    print(f'Full-batch training with lr={lr}')\n",
    "    model = LinearRegressor(num_features=X_train.shape[1], num_outputs=1).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    test_loss = train_and_evaluate(\n",
    "        model, \n",
    "        optimizer, \n",
    "        X_train, \n",
    "        y_train, \n",
    "        X_test, \n",
    "        y_test, \n",
    "        iterations=iterations, \n",
    "        train_step=autograd_train_step,\n",
    "        verbose=False\n",
    "    )\n",
    "    trained_models[\"full-batch\"][lr] = {\n",
    "        \"model\": model,\n",
    "        \"last_test_loss\": test_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print, plot, and compare the losses on the last step for each training algorithm and each learning rate. What do you observe? What does it mean? Find a proper way to present the information in the plots such that your takeaway message is clear!\n",
    "\n",
    "# print losses\n",
    "for training_type in trained_models:\n",
    "    print(f'{training_type}:')\n",
    "    for lr, data in trained_models[training_type].items():\n",
    "        print(f'  lr={lr}, last test loss={data[\"last_test_loss\"]}')\n",
    "\n",
    "# plot losses\n",
    "plt.xlabel('Learning rate')\n",
    "plt.ylabel('Last test loss')\n",
    "plt.title('Last test loss vs learning rate')\n",
    "for training_type in trained_models:\n",
    "    points = []\n",
    "    for lr, data in trained_models[training_type].items():\n",
    "        points.append((lr, data[\"last_test_loss\"]))\n",
    "    points = torch.tensor(points).detach().cpu().numpy()\n",
    "    plt.scatter(*zip(*points), label=training_type)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Benchmark full-batch gradient descent and stochastic gradient descent for different iterations ($[10, 100, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]$). Keep the learning rate fixed to $0.0001$. Print, plot, and compare the loss on the last step for each training algorithm and the different number of iterations. What do you observe? What does it mean? Find a proper way to present the information in the plots such that your takeaway message is clear!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "set_of_iterations = [10, 100, 1_000, 2_000, 3_000, 4_000, 5_000, 6_000, 7_000, 8_000, 9_000, 10_000]\n",
    "batch_size = 32\n",
    "\n",
    "trained_models_ex10 = {\"full-batch\": {}, \"batched\": {}} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched training\n",
    "\n",
    "for iterations in set_of_iterations:\n",
    "    print(f'Batched training with iterations={iterations}')\n",
    "    model_batch = LinearRegressor(num_features=X_train.shape[1], num_outputs=1).to(device)\n",
    "    optimizer_batch = torch.optim.SGD(model_batch.parameters(), lr=lr)\n",
    "    test_loss_batch = train_and_evaluate_on_batches(\n",
    "        model_batch,\n",
    "        optimizer_batch,\n",
    "        X_train, \n",
    "        y_train, \n",
    "        X_test, \n",
    "        y_test, \n",
    "        iterations=iterations, \n",
    "        batch_size=batch_size,\n",
    "        train_step=autograd_train_step,\n",
    "        verbose=False\n",
    "    )\n",
    "    trained_models_ex10[\"batched\"][iterations] = {\n",
    "        \"model\": model_batch,\n",
    "        \"last_test_loss\": test_loss_batch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full-batch training\n",
    "\n",
    "for iterations in set_of_iterations:\n",
    "    print(f'Full-batch training with iterations={iterations}')\n",
    "    model = LinearRegressor(num_features=X_train.shape[1], num_outputs=1).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    test_loss = train_and_evaluate(\n",
    "        model, \n",
    "        optimizer, \n",
    "        X_train, \n",
    "        y_train, \n",
    "        X_test, \n",
    "        y_test, \n",
    "        iterations=iterations, \n",
    "        train_step=autograd_train_step,\n",
    "        verbose=False\n",
    "    )\n",
    "    trained_models_ex10[\"full-batch\"][iterations] = {\n",
    "        \"model\": model,\n",
    "        \"last_test_loss\": test_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print, plot, and compare the losses on the last step for each training algorithm and each number of iterations. What do you observe? What does it mean? Find a proper way to present the information in the plots such that your takeaway message is clear!\n",
    "\n",
    "# print losses\n",
    "for training_type in trained_models_ex10:\n",
    "    print(f'{training_type}:')\n",
    "    for iterations, data in trained_models_ex10[training_type].items():\n",
    "        print(f'  iterations={iterations}, last test loss={data[\"last_test_loss\"]}')\n",
    "\n",
    "# plot losses\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Last test loss')\n",
    "plt.title('Last test loss vs number of iterations')\n",
    "for training_type in trained_models_ex10:\n",
    "    points = []\n",
    "    for iterations, data in trained_models_ex10[training_type].items():\n",
    "        points.append((iterations, data[\"last_test_loss\"]))\n",
    "    points = torch.tensor(points).detach().cpu().numpy()\n",
    "    plt.scatter(*zip(*points), label=training_type)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Fill in the class body to implement SGD without automatic differentiation. Do not use torch.nn.Parameter and instead implement a manual backward call. Fill in the new step function that does not use autograd but calls this custom backward call. The gradients of the two parameters can be computed by using the following formulas: $\\delta w = 2 \\frac{1}{N} \\sum_{i=1}^{N}x_i (\\hat{y}_i-y_i)$ and $\\delta b = 2 \\frac{1}{N} \\sum_{i=1}^{N}(\\hat{y}_i-y_i)$, where $N$ is the batch size and $\\hat{y}_i$ the prediction for the $i^{th}$ instance. Test the implementation with the call to training already given. Plot the predictions on the test set and compute the MSELoss and Pearson R.\n",
    "`Note:` Do not use loops for the backpropagation in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinearRegressor:\n",
    "    \"\"\"Custom linear regression class.\n",
    "    \n",
    "    Args:\n",
    "        num_features: number of features in your linear regression\n",
    "        num_outputs: number of outputs. Default: 1\n",
    "        lr: learning rate for gradient descent\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, num_features: int, num_outputs: int = 1, lr: float = 0.001):\n",
    "        self.num_features = num_features\n",
    "        self.num_outputs = num_outputs\n",
    "        self.lr = lr\n",
    "        self.w = torch.randn(num_features, num_outputs, requires_grad=False)\n",
    "        self.b = torch.randn(num_outputs, requires_grad=False)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Forward call function.\n",
    "        \n",
    "        Implement linear regression.\n",
    "        \n",
    "        Args:\n",
    "            x: data\n",
    "        \"\"\"\n",
    "        return torch.mm(x, self.w) + self.b\n",
    "    \n",
    "    def gradient_descent(self, x, y, y_hat):\n",
    "        \"\"\"Backward call function.\n",
    "        \n",
    "        Implement parameter update\n",
    "        \n",
    "        Args:\n",
    "            x: data\n",
    "            y: ground truth labels\n",
    "            y_hat: model predictions\n",
    "        \"\"\"\n",
    "        # y_hat = self(x) = x @ w + b\n",
    "        # loss = MSE(y_hat, y) = ((y_hat-y)**2).mean()\n",
    "        # d loss / d w = 2 * (y_hat - y) * d y_hat / d w = 2 * (y_hat - y) * x\n",
    "        # d loss / d b = 2 * (y_hat - y) * d y_hat / d b = 2 * (y_hat - y)\n",
    "\n",
    "        # as batched tensors\n",
    "        # (d loss / d w)[i,j] = 1/batch 1/num_outputs sum_ko  2*(y_hat[k,o]-y[k,o])*x[k,i]*delta[j,o]\n",
    "        # (d loss / d b)[j]   = 1/batch 1/num_outputs sum_ko  2*(y_hat[k,o]-y[k,o])*delta[j,o]\n",
    "        \n",
    "        # tensor notation\n",
    "        # (d loss / d w) = 1/batch x.t() @ 2*(y_hat-y)\n",
    "        # (d loss / d b) = 1/batch sum(2*(y_hat-y), axis=0)\n",
    "\n",
    "        # compute gradients\n",
    "        batch = x.shape[0]\n",
    "        dloss_dw = 1/batch * 2 * x.t() @ (y_hat-y)\n",
    "        dloss_db = 1/batch * 2 * (y_hat-y).sum(axis=0)\n",
    "\n",
    "        # update weights\n",
    "        self.w -= self.lr * dloss_dw\n",
    "        self.b -= self.lr * dloss_db\n",
    "        \n",
    "            \n",
    "    def __repr__(self):\n",
    "        return f'CustomLinearRegressor(w: {self.w.shape}, b: {self.b.shape})'\n",
    "\n",
    "custom_model = CustomLinearRegressor(8)\n",
    "print(custom_model)\n",
    "\n",
    "def custom_train_step(\n",
    "    model: torch.nn.Module, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    X_train: torch.Tensor, \n",
    "    y_train: torch.Tensor\n",
    "):\n",
    "    r\"\"\"Function to implement training with custom differentiation.\n",
    "    \n",
    "    Args:\n",
    "        model: pytorch model to be trained with custom differentiation\n",
    "        optimizer: parameter provided for compatibility and should be IGNORED.\n",
    "        X: pytorch tensor containing features\n",
    "        y: pytorch tensor containing labels\n",
    "    \"\"\"\n",
    "    pred = model(X_train)\n",
    "    model.gradient_descent(X_train, y_train, pred)\n",
    "    loss = MSEloss(pred, y_train)\n",
    "    return loss\n",
    "\n",
    "iterations = 10000\n",
    "loss = train_and_evaluate_on_batches(\n",
    "    model=custom_model, \n",
    "    optimizer=None, \n",
    "    X_train=X_train, \n",
    "    y_train=y_train, \n",
    "    X_test=X_test, \n",
    "    y_test=y_test, \n",
    "    iterations=iterations, \n",
    "    batch_size=batch_size,\n",
    "    train_step=custom_train_step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predictions on the test set and compute the MSELoss and Pearson R.\n",
    "\n",
    "y_hat_custom = custom_model(X_test)\n",
    "plt.scatter(y_test.cpu(), y_hat_custom.cpu())\n",
    "plt.xlabel('Ground truth')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Predictions vs Ground truth')\n",
    "plt.show()\n",
    "\n",
    "test_R_custom = scipy.stats.pearsonr(y_hat_custom[:,0].cpu(), y_test.cpu())[0]\n",
    "print(f'Custom Pearson R: {test_R_custom}')\n",
    "print(f'Baseline Pearson R: {R}')\n",
    "\n",
    "test_loss_custom = MSEloss(y_hat_custom, y_test)\n",
    "print(f'Custom test MSE loss: {test_loss_custom}')\n",
    "print(f'Baseline test MSE loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### notes\n",
    "\n",
    "# estimate FLOPs per training\n",
    "def estimate_flops(\n",
    "    iterations: int,\n",
    "    batch_size: int,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise2-linreg_full.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9511b72e2f0dc8789a87876b871a2c068f47e3c15d5f47ed9d92a201978a3866"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
