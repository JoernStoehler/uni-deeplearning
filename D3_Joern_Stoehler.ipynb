{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IahgcTUG6_YI"
   },
   "source": [
    "# Feed-Forward Neural Networks for Image Classification\n",
    "\n",
    "In this exercise, you will code a neural network for classifying images of digits, such as in the image below.\n",
    "\n",
    "![alt_text](https://miro.medium.com/max/800/1*LyRlX__08q40UJohhJG9Ow.png)\n",
    "\n",
    "We will work with the MNIST dataset, that is comprised of 60,000 training images and 10,000 test images. Each image is 28x28 pixels, with a grayscale value between 0 and 255. Therefore, the input to our neural network will be a minibatch of vectors, each of shape [784] ($28 \\cdot 28$). The label is an integer $\\in [0,9]$. For more information about the data have a look at the [torchvision documentation](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST).\n",
    "\n",
    "Our model will be a fully-connected neural network with 2 hidden layers, each with 64 units and a ReLU activation function. On top of the hidden layers we will have the standard classification head: a fully-connected layer with 10 units. The network will be trained with the cross-entropy loss.\n",
    "\n",
    "Today's exercise sheet can be broken down in 3 main sections:\n",
    "\n",
    "A. Setting up the model architecture\n",
    "\n",
    "B. Training and evaluating the baseline\n",
    "\n",
    "C. Optimizing the model to improve performance\n",
    "\n",
    "This conceptual split is something typically done when optimizing deep learning architectures.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Create your own linear layer torch module, which randomly initializes the variables W and b for a fully connected layer with `torch.nn.Parameter()` using `torch.nn.init.kaiming_uniform_` initialization for the weights. Implement the forward()-function for your module. Input to the class should be the number of input features and the number of the  output features. Implement the `__init__` and the `forward` methods. Use matrix multiplications and add the bias to implement the forward propagation.\n",
    "2. Fill in the following simple test for the implementation of `MyLinearLayer`. You should only test whether your implementation can handle different input/output/batch sizes and return the correct output size. Test it for `input_sizes=[1, 2, 4]`, `output_sizes=[1, 2, 4]`, and `batch_sizes=[1, 2, 4]`. Do it as follows: a) Create a random input tensor of size `(batch_size, input_size)`, b) Instantiate `MyLinearLayer` with `input_size, output_size`, c) Run the sample input through the layer, d) Write an `assert` to check if the shape matches the expected one `(batch_size, output_size)`.\n",
    "3. Fill in the body of the `MyFeedforwardNN` class to implement a feed-forward neural network to implement a network with two hidden layers. Each hidden layer should have a size of 64 The class should accept as input: a) dimensionality of input features, b) the size of the output; the network should have an output layer mapping from the last hidden activations to the output; this layer should have no activation, c) the activation function for the hidden layers (e.g. `torch.sigmoid`).\n",
    "4. Fill in the following simple test for the implementation of `MyFeedforwardNN`. Use the same test you have for `MyLinearLayer`. Use `torch.sigmoid` as the activation of the hidden layers.\n",
    "5. Use `torch.utils.data.DataLoader` to wrap your training and test datasets. The data loader should have a `batch_size` of 32 and use `shuffle=True` for training and `shuffle=False` for testing. This objects are used to automatically iterate over your data.\n",
    "6. Instantiate a feed-forward neural network using your model. Use a `torch.sigmoid` hidden activation. Do not forget to transfer it to device afterwards. Hint: see the output of the given data loading cell for the correct input and output sizes.\n",
    "7. Fill in the following function to run the model over all data and store the predictions in the given tensors. This is the function you will later use to evaluate your model.\n",
    "8. Fill in the following function to compute the accuracy over an entire dataset given the model predictions (as logits) and the ground truth labels. For this you need to map the model outputs (which are multi-dimensional) back to a one-dimensional tensor corresponding to the predicted classes.\n",
    "9. Fill in the following function to train and evaluate the model on the MNIST training and test set, respectively. The function should call `single_model_step` with the correct `training` flag each time (it is very important to set it to `False` for evaluation). The training should run for the given number of `epochs`. At the end of **each epoch**, you should optionally (controlled by the `verbose` flag) print: a) the average training loss and b) the average test loss. Additionally, you should print (again controlled by the `verbose` flag)  the accuracy on the test set **after the last epoch**. This accuracy should also be returned by the function. Once you have filled in the function body, instantiate your model anew, wrap it in an `SGD` optimizer with a learning rate of 0.001, and run the training for 3 epochs.\n",
    "10. Create a for loop to instantiate model with different hidden activations, train it for 10 epochs, and store its accuracy in a list. Afterwards, create a plot with the accuracy on the y-axis and the x-axis corresponding to its activation function (the tick labels). Use: `torch.relu`, `torch.sigmoid`, `torch.tanh`, and `torch.selu`. Keep the parameters of the optimizer the same as before (but instantiate it anew to wrap the new model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import typing\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "print(np.__version__)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed = seed\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# set up model to use GPU if available\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Setting up the model architecture\n",
    "\n",
    "In this section, you will implement the necessary steps to set up a (modular) feed-forward neural network architecture that you will later use to solve today's task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create your own linear layer torch module, which randomly initializes the variables W and b for a fully connected layer with `torch.nn.Parameter()` using `torch.nn.init.kaiming_uniform_` initialization for the weights. Implement the forward()-function for your module. Input to the class should be the number of input features and the number of the  output features. Implement the `__init__` and the `forward` methods. Use matrix multiplications and add the bias to implement the forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fill in the following simple test for the implementation of `MyLinearLayer`. You should only test whether your implementation can handle different input/output/batch sizes and return the correct output size. Test it for `input_sizes=[1, 2, 4]`, `output_sizes=[1, 2, 4]`, and `batch_sizes=[1, 2, 4]`. Do it as follows: a) Create a random input tensor of size `(batch_size, input_size)`, b) Instantiate `MyLinearLayer` with `input_size, output_size`, c) Run the sample input through the layer, d) Write an `assert` to check if the shape matches the expected one `(batch_size, output_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sizes = [1, 2, 4]\n",
    "output_sizes = [1, 2, 4]\n",
    "batch_sizes = [1, 2, 4]\n",
    "\n",
    "for input_size in input_sizes:\n",
    "    for output_size in output_sizes:\n",
    "        for batch_size in batch_sizes:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fill in the body of the `MyFeedforwardNN` class to implement a feed-forward neural network to implement a network with two hidden layers. Each hidden layer should have a size of 64 The class should accept as input: a) dimensionality of input features, b) the size of the output; the network should have an output layer mapping from the last hidden activations to the output; this layer should have no activation, c) the activation function for the hidden layers (e.g. `torch.sigmoid`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFeedforwardNN(torch.nn.Module):\n",
    "    r\"\"\"Feed-forward neural network generator.\n",
    "    \n",
    "    Args:\n",
    "        input_size: dimensionality of input features\n",
    "        output_size: dimensionality of output\n",
    "        hidden_activation: function to be used as activation for the hidden layers\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size: int,\n",
    "        output_size: int, \n",
    "        hidden_activation: typing.Callable\n",
    "    ):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Fill in the following simple test for the implementation of `MyFeedforwardNN`. Use the same test you have for `MyLinearLayer`. Use `torch.sigmoid` as the activation of the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sizes = [1, 2, 4]\n",
    "output_sizes = [1, 2, 4]\n",
    "batch_sizes = [1, 2, 4]\n",
    "\n",
    "for input_size in input_sizes:\n",
    "    for output_size in output_sizes:\n",
    "        for batch_size in batch_sizes:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Training and evaluating the baseline\n",
    "\n",
    "In this section, you will implement all of the necessary steps to train and evaluate your model for multiclass digit classification based on image data. You will first implement suitable functions to generate the predictions of your model on an entire dataset (where you will use appropriate `torch` utilities for handling data iteration) and then compute two standard evaluation metrics: the accuracy and the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the datasets using `torchvision`. This gives us two `torch.data.utils.Dataset` objects (one for training and one for testing) that you can later use to get your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torch.nn.Flatten(start_dim=0)\n",
    "    ]), \n",
    "    download = True,            \n",
    ")\n",
    "print(train_dataset)\n",
    "x, y = train_dataset[0]\n",
    "print(f'Feature shape: {x.shape}')\n",
    "print(f'Num classes: {len(train_dataset.classes)}')\n",
    "print(f'Classes: {train_dataset.classes}')\n",
    "print()\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = False,                         \n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torch.nn.Flatten(start_dim=0)\n",
    "    ]), \n",
    "    download = True,            \n",
    ")\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use `torch.utils.data.DataLoader` to wrap your training and test datasets. The data loader should have a `batch_size` of 32 and use `shuffle=True` for training and `shuffle=False` for testing. This objects are used to automatically iterate over your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Instantiate a feed-forward neural network using your model. Use a `torch.sigmoid` hidden activation. Do not forget to transfer it to device afterwards. Hint: see the output of the given data loading cell for the correct input and output sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Fill in the following function to run the model over all data and store the predictions in the given tensors. This is the function you will later use to evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_model(\n",
    "    model: torch.nn.Module, \n",
    "    loader: torch.utils.data.DataLoader, \n",
    "    device: str\n",
    "):\n",
    "    r\"\"\"Evaluate model on dataloader.\n",
    "    \n",
    "    Iterates over loader until it is exhausted \n",
    "    and stores the model output (logits) and labels\n",
    "    in tensors.\n",
    "    \n",
    "    Args:\n",
    "        model: model to be evaluated\n",
    "        loader: dataloader containing data to evaluate model on\n",
    "        device: string denoting device on which to run evaluation\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: model outputs (logits)\n",
    "        torch.Tensor: ground truth labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    logits = torch.zeros((len(loader.dataset), len(loader.dataset.classes)))\n",
    "    labels = torch.zeros((len(loader.dataset)))\n",
    "    raise NotImplementedError\n",
    "\n",
    "logits, labels = inference_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Fill in the following function to compute the accuracy over an entire dataset given the model predictions (as logits) and the ground truth labels. For this you need to map the model outputs (which are multi-dimensional) back to a one-dimensional tensor corresponding to the predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(labels, logits):\n",
    "    r\"\"\"Compute accuracy.\n",
    "    \n",
    "    Args:\n",
    "        labels: ground truth labels\n",
    "        logits: model predictions\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: single accuracy value\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \n",
    "get_accuracy(labels, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we provide you with a single model step which can be used either for training (where the gradients are computed and autograd is called) or evaluation. This is merely an extension of the `autograd_step` function you implemented yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_model_step(\n",
    "    model: torch.nn.Module, \n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_function: typing.Callable,\n",
    "    training: bool,\n",
    "    X: torch.Tensor, \n",
    "    y: torch.Tensor,\n",
    "    device: str\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"Single model training/evaluation step.\n",
    "\n",
    "    Args:\n",
    "        model: pytorch model to be trained\n",
    "        optimizer: optimizer wrapping pytorch model\n",
    "        loss_function: loss function\n",
    "        training: flag controlling whether this is a training\n",
    "            or an evaluation step\n",
    "        X: pytorch tensor containing features\n",
    "        y: pytorch tensor containing labels\n",
    "        device: device to where model is located\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: loss at current step\n",
    "    \"\"\"\n",
    "    pred = model(X.to(device))\n",
    "    loss = loss_function(pred, y.to(device))\n",
    "    if training:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss.cpu().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Fill in the following function to train and evaluate the model on the MNIST training and test set, respectively. The function should call `single_model_step` with the correct `training` flag each time (it is very important to set it to `False` for evaluation). Use a torch-implementation of `cross-entropy` as loss function. The training should run for the given number of `epochs`. At the end of **each epoch**, you should optionally (controlled by the `verbose` flag) print: a) the average training loss and b) the average test loss. Additionally, you should print (again controlled by the `verbose` flag)  the accuracy on the test set **after the last epoch**. This accuracy should also be returned by the function. Once you have filled in the function body, instantiate your model anew, wrap it in an `SGD` optimizer with a learning rate of 0.001, and run the training for 3 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    model: torch.nn.Module, \n",
    "    loss_function: typing.Callable, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    epochs: int,\n",
    "    device: str,\n",
    "    verbose: bool,\n",
    "):\n",
    "    r\"\"\"Run training and evaluation.\n",
    "    \n",
    "    Args:\n",
    "        model: pytorch model to be trained\n",
    "        loss_function: loss function\n",
    "        optimizer: optimizer wrapping pytorch model\n",
    "        train_loader: dataloader containing training data\n",
    "        test_loader: dataloader containing training data\n",
    "        epochs: number of epochs for which to train model\n",
    "        device: device to where model is located\n",
    "        verbose: flag controlling whether to print information\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: accuracy on test set after last epoch\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Optimizing the model to improve performance\n",
    "\n",
    "Now that you have successfully trained your model, the next step is to try out different methods for optimizing its performance. There is a huge variety of different optimizations that you can apply on your model. For this exercise, you will adapt the activation function of the hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a for loop to instantiate model with different hidden activations, train it for 10 epochs, and store its accuracy in a list. Afterwards, create a plot with the accuracy on the y-axis and the x-axis corresponding to its activation function (the tick labels). Use: `torch.relu`, `torch.sigmoid`, `torch.tanh`, and `torch.selu`. Keep the parameters of the optimizer the same as before (but instantiate it anew to wrap the new model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise3-dnn-solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "9511b72e2f0dc8789a87876b871a2c068f47e3c15d5f47ed9d92a201978a3866"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
