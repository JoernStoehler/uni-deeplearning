{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KByC9YAElJzX"
   },
   "source": [
    "# D5 : Introduce Convolutional Neural Networks\n",
    "\n",
    "Today we will learn how to make a Convolutional Neural Network (CNN) for classifying images in the CIFAR-10 dataset.\n",
    "\n",
    "To start with, we will briefly introduce\n",
    "\n",
    "\n",
    "*   why CNN for image classification\n",
    "\n",
    "*   CNN pipeline, convolutional layer, and max pooling layer\n",
    "\n",
    "*   CNN applications\n",
    "\n",
    "*   CIFAR-10 dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pS_QKbWXTyI5"
   },
   "source": [
    "## Why CNN\n",
    "A convolutional neural network (CNN, or ConvNet), is an architecture commonly used for deep learning. The use of CNNs for deep learning has become increasingly popular due to three important factors:\n",
    "\n",
    "*   It eliminates the need for manual feature extraction, i.e., the features are learnt directly by the CNN.\n",
    "\n",
    "*   It produces state-of-the-art recognition results.\n",
    "\n",
    "*   CNNs can be retrained for new recognition tasks and allow for building on pre-existing networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaNr67uGMZ-L"
   },
   "source": [
    "A convolutional neural network can have tens or hundreds of layers that each learn to detect different features of an image. Filters are applied to each training image at different resolutions and the output of each convolved image is used as the input to the next layer. The filters can start as very simple features, such as brightness and edges, and increase in complexity to features that uniquely define the object as the layers progress.\n",
    "\n",
    "\n",
    "CNNs for Image classification:\n",
    "\n",
    "One method for creating a convolutional neural network to perform image classification is to train a network from scratch. The architect is required to define the number of layers, the learning weights, and number of filters, along with other tunable parameters. Training an accurate model from scratch also requires massive amounts of data, on the order of millions of samples, which can take an immense amount of time to train.\n",
    "\n",
    "A common alternative to training a CNN from scratch is to use a pre-trained model to automatically extract features from a new data set. This method, called transfer learning, is a convenient way to apply deep learning without a huge data set and long computation and training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-9Hzznxslr5"
   },
   "source": [
    "## CIFAR-10 database\n",
    "CIFAR-10 classification is a common benchmark problem in machine learning. The problem is to classify RGB 32*32 pixel images across 10 categories, i.e., airplane, automobile, bird, \n",
    "cat, deer, dog, frog, horse, ship, and truck. The CIFAR-10 dataset consists of 60000 images, with 6000 images per class. There are 50000 for training and the rest 10000 for test.\n",
    "For more details refer to the CIFAR-10 (https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "\n",
    "in which, X_train, X_test are arrays of RBG image data with shape (num_samples, 3, 32, 32), and y_train, y_test are arrays of category labels (0-9) with shape (num_samples, 1).\n",
    "\n",
    "In PyTorch you can load CIFAR-10 with `torchvision.datasets.CIFAR10()`, which returns a datset for either the training or the test data, depending on the parameter `train=True` or `train=False`. Tha data hast the shape (num_samples, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2uf6Wsvpz0s"
   },
   "source": [
    "## Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kk1Jm0oh2KEJ",
    "outputId": "9312c4ab-c7e0-413d-d55d-5faeb00b4e6c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import typing\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.nn import BatchNorm2d, Conv2d, MaxPool2d, Flatten, Linear, Dropout\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(np.__version__)\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "\n",
    "# set up model to use GPU if available\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNC9rc4HTeew"
   },
   "source": [
    "## Exercise 1: Build a CNN with PyTorch for CIFAR-10 classification (with the following structure as depicted in the following figure)\n",
    "![cnn pipeline](https://cdn-images-1.medium.com/fit/t/1600/480/1*vkQ0hXDaQv57sALXAJquxA.jpeg)\n",
    "\n",
    "Steps: (No steps. First, brainstorm-time to build your own CNN)\n",
    "No need to implement the convolution computation, you may use `torch.nn.Conv2d` and `torch.nn.MaxPool2d` directly.\n",
    "But still, you need to set (initialise) weights and bias properly and give correct shapes of tensors to make it rock.\n",
    "\n",
    "### Your goal: more than 70% accuracy on the whole test set (be over 70% for 3+ epochs in a row).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171,
     "referenced_widgets": [
      "68adb56b01054aae800e0378d8283d54",
      "690e2a29318d4fd3ae76d9a58e9aafb0",
      "e164487f620c46afad375ac433481dbf",
      "51e3ba850f854eb1a2c818b34eff1596",
      "5b90f2e9f74d48cf9edc7d231e1cb4f7",
      "9077cf58190145678c1baca2317fa12b",
      "91f30ac6685d40fdb599df4e395f7dc2",
      "5f924aba3790451b9110d89b2b62ceb7",
      "cec8c60319c6475d926ef6646efbf106",
      "ae2cbaaf947a4fefbab97da0f38e9d2c",
      "57d81ba0474948d88e57d315a38fac2b"
     ]
    },
    "id": "xelDLaqv2KEN",
    "outputId": "b832711c-218b-4b2c-efe6-a164ff58ca46"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load the CIFAR-10 data\n",
    "train_data = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", \n",
    "    train=True, \n",
    "    transform=torchvision.transforms.ToTensor(), \n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", \n",
    "    train=False, \n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "X_train, y_train = torch.tensor(train_data.data), torch.tensor(train_data.targets)\n",
    "X_test, y_test = torch.tensor(test_data.data), torch.tensor(test_data.targets)\n",
    "\n",
    "assert X_train.shape == (50000, 32, 32, 3)\n",
    "assert X_test.shape == (10000, 32, 32, 3)\n",
    "assert y_train.shape == (50000,)\n",
    "assert y_test.shape == (10000,)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9pcoJSzpz00"
   },
   "source": [
    "## Provided code from the days before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvpKCHE7pz00"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "def single_model_step(\n",
    "    model: torch.nn.Module, \n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_function: typing.Callable,\n",
    "    training: bool,\n",
    "    X: torch.Tensor, \n",
    "    y: torch.Tensor,\n",
    "    device: str\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"Single model training/evaluation step.\n",
    "\n",
    "    Args:\n",
    "        model: pytorch model to be trained\n",
    "        optimizer: optimizer wrapping pytorch model\n",
    "        loss_function: loss function\n",
    "        training: flag controlling whether this is a training\n",
    "            or an evaluation step\n",
    "        X: pytorch tensor containing features\n",
    "        y: pytorch tensor containing labels\n",
    "        device: device to where model is located\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: loss at current step\n",
    "    \"\"\"\n",
    "    pred = model(X.to(device))\n",
    "    loss = loss_function(pred, y.to(device))\n",
    "    if training:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss.cpu().item()\n",
    "\n",
    "def inference_model(\n",
    "    model: torch.nn.Module, \n",
    "    loader: torch.utils.data.DataLoader,\n",
    "    num_classes,\n",
    "    device: str\n",
    "):\n",
    "    r\"\"\"Evaluate model on dataloader.\n",
    "    \n",
    "    Iterates over loader until it is exhausted \n",
    "    and stores the model output (logits) and labels\n",
    "    in tensors.\n",
    "    \n",
    "    Args:\n",
    "        model: model to be evaluated\n",
    "        loader: dataloader containing data to evaluate model on\n",
    "        device: string denoting device on which to run evaluation\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: model outputs (logits)\n",
    "        torch.Tensor: ground truth labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    logits = torch.zeros((len(loader.dataset), num_classes))\n",
    "    labels = torch.zeros((len(loader.dataset)))\n",
    "    with torch.no_grad():\n",
    "        for index, (X, y) in enumerate(loader):\n",
    "            logits[index*BATCH_SIZE:(index+1)*BATCH_SIZE, :] = model(X.to(device)).cpu()\n",
    "            labels[index*BATCH_SIZE:(index+1)*BATCH_SIZE] = y\n",
    "    return logits, labels\n",
    "\n",
    "def get_accuracy(labels, logits):\n",
    "    r\"\"\"Compute accuracy.\n",
    "    \n",
    "    Args:\n",
    "        labels: ground truth labels\n",
    "        logits: model predictions\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: single accuracy value\n",
    "    \"\"\"\n",
    "    prediction_indices = torch.argmax(logits, axis=-1)\n",
    "    correct_predictions = labels == prediction_indices\n",
    "    accuracy = correct_predictions.sum() / len(correct_predictions)\n",
    "    return accuracy.item()\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(\n",
    "    model: torch.nn.Module, \n",
    "    loss_function: typing.Callable, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    num_classes: int,\n",
    "    epochs: int,\n",
    "    device: str,\n",
    "    verbose: bool,\n",
    "):\n",
    "    r\"\"\"Run training and evaluation.\n",
    "    \n",
    "    Args:\n",
    "        model: pytorch model to be trained\n",
    "        loss_function: loss function\n",
    "        optimizer: optimizer wrapping pytorch model\n",
    "        train_loader: dataloader containing training data\n",
    "        test_loader: dataloader containing training data\n",
    "        num_classes: number of classes for the classification task\n",
    "        epochs: number of epochs for which to train model\n",
    "        device: device to where model is located\n",
    "        verbose: flag controlling whether to print information\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: accuracy on test set after last epoch\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for train_steps, (X, y) in enumerate(train_loader):\n",
    "            train_loss += single_model_step(\n",
    "                model=model, \n",
    "                loss_function=loss_function, \n",
    "                optimizer=optimizer, \n",
    "                training=True,\n",
    "                X=X, \n",
    "                y=y,\n",
    "                device=device\n",
    "            )\n",
    "        train_loss /= train_steps\n",
    "        \n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for test_steps, (X, y) in enumerate(test_loader):\n",
    "                test_loss += single_model_step(\n",
    "                    model=model, \n",
    "                    loss_function=loss_function, \n",
    "                    optimizer=optimizer, \n",
    "                    training=False,\n",
    "                    X=X, \n",
    "                    y=y,\n",
    "                    device=device\n",
    "                )\n",
    "        test_loss /= test_steps\n",
    "        \n",
    "        \n",
    "        print(f\"\\n---------- EPOCH {epoch + 1} ------------\")\n",
    "        print(f\"Average Train Loss: {train_loss}\")\n",
    "        print(f\"Average Test Loss: {test_loss}\")\n",
    "        logits, labels = inference_model(\n",
    "            model=model, \n",
    "            loader=test_loader,\n",
    "            num_classes=num_classes,\n",
    "            device=device\n",
    "        )\n",
    "        accuracy = get_accuracy(\n",
    "            labels=labels,\n",
    "            logits=logits\n",
    "        )\n",
    "        print(f\"Test Accuracy: {accuracy}\")\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdD-Pzsipz02"
   },
   "source": [
    "## Your turn, have fun! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ax58kWh08rzU"
   },
   "source": [
    "Use the data loaders `train_loader` and `test_loader` from above for your training. The only thing you should have to do is to create your own model. For training and evaluating it you can simply use the provided functions from the last days (also incorporated above). Again a short reminder: you may use `torch.nn.Conv2d` and `torch.nn.MaxPool2d` directly. However, don't forget to set (initialise) weights and bias properly and give correct shapes of tensors.\n",
    "Achieve a test accuracy of more than 70% on the whole test set for 3+ epochs in a row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mCXWJkD2KEQ"
   },
   "outputs": [],
   "source": [
    "# Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SevLyxhQsljC"
   },
   "source": [
    "## 2. Shape and number of parameters\n",
    "\n",
    "Compute the shapes of your convolutional and max pooling layers as well as the number of trainable parameters of your convolutions and fully-connected layers. Use the formulas below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxx9kxtnUkvO"
   },
   "source": [
    "### Formulas for the calculation of the shapes and trainable parameters\n",
    "\n",
    "#### Number of trainable parameters:\n",
    "\n",
    "n_params_dense = n_input*n_output + n_output  , i.e.: n_weight_matrix + n_bias\n",
    "\n",
    "\n",
    "n_params_conv = kernel_size_h \\* kernel_size_w \\* n_filters_in \\* n_filters_out + n_filters_out \n",
    "\n",
    "Every region of (kernelwidth x kernelheight x input_filters) shares weights. Output for every region is (n_filters_out), afterwards, bias is added, therefore + n_filters_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGZ8RKNoHZn0"
   },
   "source": [
    "#### Output shape of a concolutional layer:\n",
    "1. output_width = (input_width - kernel_size + 2 \\* amount_zero_padding) / stride + 1\n",
    "2. output_height = (input_height - kernel_size + 2 \\* amount_zero_padding) / stride + 1\n",
    "3. output_depth = n_filters\n",
    "\n",
    "#### Output shape of a max pooling layer:\n",
    "1. output_width = (input_width - kernel_size) / stride + 1\n",
    "2. output_height = (input_height - kernel_size) / stride + 1\n",
    "3. output_depth = n_filters\n",
    "\n",
    "If the result is a decimal point, it is rounded down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3ALdWrPIemh"
   },
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "9511b72e2f0dc8789a87876b871a2c068f47e3c15d5f47ed9d92a201978a3866"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "51e3ba850f854eb1a2c818b34eff1596": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae2cbaaf947a4fefbab97da0f38e9d2c",
      "placeholder": "​",
      "style": "IPY_MODEL_57d81ba0474948d88e57d315a38fac2b",
      "value": " 170498071/170498071 [00:01&lt;00:00, 88430967.72it/s]"
     }
    },
    "57d81ba0474948d88e57d315a38fac2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b90f2e9f74d48cf9edc7d231e1cb4f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f924aba3790451b9110d89b2b62ceb7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68adb56b01054aae800e0378d8283d54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_690e2a29318d4fd3ae76d9a58e9aafb0",
       "IPY_MODEL_e164487f620c46afad375ac433481dbf",
       "IPY_MODEL_51e3ba850f854eb1a2c818b34eff1596"
      ],
      "layout": "IPY_MODEL_5b90f2e9f74d48cf9edc7d231e1cb4f7"
     }
    },
    "690e2a29318d4fd3ae76d9a58e9aafb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9077cf58190145678c1baca2317fa12b",
      "placeholder": "​",
      "style": "IPY_MODEL_91f30ac6685d40fdb599df4e395f7dc2",
      "value": "100%"
     }
    },
    "9077cf58190145678c1baca2317fa12b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91f30ac6685d40fdb599df4e395f7dc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae2cbaaf947a4fefbab97da0f38e9d2c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cec8c60319c6475d926ef6646efbf106": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e164487f620c46afad375ac433481dbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f924aba3790451b9110d89b2b62ceb7",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cec8c60319c6475d926ef6646efbf106",
      "value": 170498071
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
